{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Obtaining dependency information for streamlit from https://files.pythonhosted.org/packages/d3/96/9251b421d0a1c7d625a82a04bea56b8a9830c785940ec16db454b85c6db7/streamlit-1.29.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading streamlit-1.29.0-py2.py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from streamlit) (5.2.0)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Obtaining dependency information for blinker<2,>=1.0.0 from https://files.pythonhosted.org/packages/fa/2a/7f3714cbc6356a0efec525ce7a0613d581072ed6eb53eb7b9754f33db807/blinker-1.7.0-py3-none-any.whl.metadata\n",
      "  Downloading blinker-1.7.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit)\n",
      "  Obtaining dependency information for cachetools<6,>=4.0 from https://files.pythonhosted.org/packages/a2/91/2d843adb9fbd911e0da45fbf6f18ca89d07a087c3daa23e955584f90ebf4/cachetools-5.3.2-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from streamlit) (8.0.4)\n",
      "Requirement already satisfied: importlib-metadata<7,>=1.4 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: numpy<2,>=1.19.3 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from streamlit) (1.23.4)\n",
      "Requirement already satisfied: packaging<24,>=16.8 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from streamlit) (23.2)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from streamlit) (2.0.3)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from streamlit) (9.4.0)\n",
      "Collecting protobuf<5,>=3.20 (from streamlit)\n",
      "  Obtaining dependency information for protobuf<5,>=3.20 from https://files.pythonhosted.org/packages/fe/6b/7f177e8d6fe4caa14f4065433af9f879d4fab84f0d17dcba7b407f6bd808/protobuf-4.25.1-cp310-abi3-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.25.1-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: pyarrow>=6.0 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from streamlit) (11.0.0)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.3 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from streamlit) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from streamlit) (2.31.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from streamlit) (13.7.0)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from streamlit) (8.2.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from streamlit) (4.9.0)\n",
      "Collecting tzlocal<6,>=1.1 (from streamlit)\n",
      "  Obtaining dependency information for tzlocal<6,>=1.1 from https://files.pythonhosted.org/packages/97/3f/c4c51c55ff8487f2e6d0e618dba917e3c3ee2caae6cf0fbb59c9b1876f2e/tzlocal-5.2-py3-none-any.whl.metadata\n",
      "  Downloading tzlocal-5.2-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting validators<1,>=0.2 (from streamlit)\n",
      "  Obtaining dependency information for validators<1,>=0.2 from https://files.pythonhosted.org/packages/3a/0c/785d317eea99c3739821718f118c70537639aa43f96bfa1d83a71f68eaf6/validators-0.22.0-py3-none-any.whl.metadata\n",
      "  Downloading validators-0.22.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Obtaining dependency information for gitpython!=3.1.19,<4,>=3.0.7 from https://files.pythonhosted.org/packages/8d/c4/82b858fb6483dfb5e338123c154d19c043305b01726a67d89532b8f8f01b/GitPython-3.1.40-py3-none-any.whl.metadata\n",
      "  Downloading GitPython-3.1.40-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
      "     ---------------------------------------- 0.0/4.8 MB ? eta -:--:--\n",
      "     ----- ---------------------------------- 0.6/4.8 MB 20.5 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 2.1/4.8 MB 26.9 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 4.0/4.8 MB 31.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 4.8/4.8 MB 30.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from streamlit) (6.3.2)\n",
      "Requirement already satisfied: watchdog>=2.1.5 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from streamlit) (2.1.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.17.3)\n",
      "Requirement already satisfied: toolz in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Obtaining dependency information for gitdb<5,>=4.0.1 from https://files.pythonhosted.org/packages/fd/5b/8f0c4a5bb9fd491c277c21eff7ccae71b47d43c4446c9d0c6cff2fe8c2c4/gitdb-4.0.11-py3-none-any.whl.metadata\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from importlib-metadata<7,>=1.4->streamlit) (3.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2023.7.22)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.15.1)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Obtaining dependency information for smmap<6,>=3.0.1 from https://files.pythonhosted.org/packages/a7/a5/10f97f73544edcdef54409f1d839f6049a0d79df68adbc1ceb24d1aaca42/smmap-5.0.1-py3-none-any.whl.metadata\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (22.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\maxlo\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.0)\n",
      "Downloading streamlit-1.29.0-py2.py3-none-any.whl (8.4 MB)\n",
      "   ---------------------------------------- 0.0/8.4 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.4/8.4 MB 44.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.0/8.4 MB 37.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 4.6/8.4 MB 37.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.3/8.4 MB 36.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.0/8.4 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.4/8.4 MB 33.5 MB/s eta 0:00:00\n",
      "Downloading blinker-1.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
      "   ---------------------------------------- 0.0/190.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 190.6/190.6 kB ? eta 0:00:00\n",
      "Downloading protobuf-4.25.1-cp310-abi3-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 413.4/413.4 kB ? eta 0:00:00\n",
      "Downloading tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "   ---------------------------------------- 0.0/62.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 62.7/62.7 kB ? eta 0:00:00\n",
      "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: validators, tzlocal, smmap, protobuf, cachetools, blinker, pydeck, gitdb, gitpython, streamlit\n",
      "Successfully installed blinker-1.7.0 cachetools-5.3.2 gitdb-4.0.11 gitpython-3.1.40 protobuf-4.25.1 pydeck-0.8.1b0 smmap-5.0.1 streamlit-1.29.0 tzlocal-5.2 validators-0.22.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import tiktoken\n",
    "import gradio as gr\n",
    "from langchain import OpenAI, PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "load_dotenv()\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import streamlit as st\n",
    "import tempfile\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"OPENAI_API_KEY\"]=openai_api_key\n",
    "def num_tokens_from_string(string:str, encoding_name:str=\"cl100k_base\") -> int:\n",
    "  #Returns the Number of Token in a text string\n",
    "  encoding = tiktoken.get_encoding(encoding_name)\n",
    "  num_tokens=len(encoding.encode(string))\n",
    "  return num_tokens\n",
    "# !curl https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All_Technical_Report.pdf -o 2023_GPT4All_Technical_Report.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "13341\n"
     ]
    }
   ],
   "source": [
    "loader=PyPDFLoader(\"2023-09 Textbooks Are All You Need II - phi-1.5 technical report.pdf\")\n",
    "docs=loader.load_and_split()\n",
    "print(len(docs))\n",
    "total_tokens = 0\n",
    "for i in range(len(docs)):\n",
    "    total_tokens = total_tokens + num_tokens_from_string(str(docs[i]))\n",
    "print(total_tokens)\n",
    "summaries = []\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "llm=ChatOpenAI(model_name=\"gpt-4-1106-preview\")\n",
    "# llm=ChatOpenAI(model_name=\"gpt-3.5-turbo-1106\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Summary of All Pages:\n",
      " Page 1: The \"Textbooks Are All You Need II: phi-1.5 technical report\" by Yuanzhi Li et al. describes the development of a new 1.3 billion parameter language model, phi-1.5, which demonstrates performance on par with much larger models in natural language tasks and excels in complex reasoning. The model is designed to generate high-quality textbook-like data, improving upon issues like hallucinations and bias found in models trained on web data. Phi-1.5 is open-source and outperforms many large models, particularly in multi-step reasoning tasks like mathematics and coding. The report includes benchmark comparisons with other models and datasets, indicating phi-1.5's strengths and noting that performance can be influenced by the model's memorized knowledge. The report's findings are based on the authors' evaluations and were published by Microsoft Research on September 11, 2023.\n",
      "\n",
      " Page 2: Large Language Models (LLMs) like GPT-4, which include models with hundreds of billions of parameters, have significantly improved natural language processing capabilities. However, the large scale of these models raises economic, scientific, and ethical concerns due to their cost, energy consumption, and governance challenges. Research is now focusing on whether smaller models can achieve similar advanced capabilities to make AI more accessible and manageable. A new model called phi-1.5, with 1.3 billion parameters, demonstrates common sense reasoning on par with much larger models, despite being trained on a smaller dataset consisting of 30 billion tokens, including synthetically generated data to reduce toxicity and bias. Phi-1.5 has been made open-source to encourage research on AI reasoning and to address issues such as in-context learning and reducing errors. The model is more efficient for experimentation, requiring less computational resources for training and inference.\n",
      "\n",
      " Page 3: Table 1 of the report details the efficiency of various AI models on an Nvidia A100-80G GPU using 2048 length context and fp16 precision. The Llama-7B model expends 80,000 GPU hours for processing, achieves a speed of 14ms per token, consumes 18G memory, and supports datasets up to 1 trillion tokens. The phi-1.5 model, relatively smaller with 1.3 billion parameters, requires 1,500 GPU hours, operates below 3ms per token, uses 3.5G memory, and can handle datasets of 30 billion tokens, with a training capacity for 150 billion tokens. Its variant, phi-1.5-web, also with 1.3 billion parameters, demands 3,000 GPU hours, maintains under 3ms per token performance, utilizes 3.5G memory, and can work with 100 billion token datasets, training up to 300 billion tokens.\n",
      "\n",
      " Page 4: The technical specifications detail the development of the model phi-1.5, which builds upon the phi-1 model's Transformer-based structure, featuring enhancements such as flash-attention and a new tokenizer. Phi-1.5's training data merges 7B tokens from phi-1 with an additional 20B tokens of synthetic data across 20K topics, aimed at improving common sense reasoning and general knowledge. The creation of a synthetic dataset of 20 billion tokens, alongside a 6 billion token code dataset, emphasizes the importance of strategic data curation to achieve reliable AI training.\n",
      "\n",
      "To evaluate the effectiveness of web data versus synthetic data, phi-1.5 was trained using a blend of both, while two variants, phi-1.5-web-only and phi-1.5-web, were trained solely on filtered web data and a mix of web and synthetic data, respectively. Both models were trained without instruction fine-tuning or RLHF and can perform question-answering tasks. The report underscores the significance of high-quality datasets in AI research and suggests that the ability to create synthetic datasets will become a crucial technical skill. This summary is based on the document \"Textbooks Are All You Need II - phi-1.5\" from 2023.\n",
      "\n",
      " Page 5: Various machine learning models were evaluated on natural language processing tasks involving common sense reasoning, language understanding, mathematics, and coding. Among these models, Vicuna-13B generally exhibited the highest accuracy, with Llama2-7B and others also performing strongly. The phi-1.5 model demonstrated comparable results to larger models despite its smaller size and limited web data. In particular, the web-trained versions of phi-1.5 models showed notable improvements and competitive performance. Performance varied across tasks, with some models like GPT-Neo-2.7B, GPT2-XL-1.5B, and OPT-1.3B lagging behind others. The phi-1.5-web version achieved the highest score among the phi models in language understanding and knowledge benchmarks.\n",
      "\n",
      " Page 6: The text outlines the evaluation of AI models' reasoning abilities through mathematical and coding benchmarks, with the phi-1.5 model excelling in coding tasks and displaying improved reasoning with web data included (phi-1.5-web). The model's coding skills are comparable to those of a coding-specific model (phi-1), suggesting the benefits of training with high-quality data. However, multi-task training can reduce accuracy, particularly in models with fewer parameters. The discussion also addresses the challenge of mitigating toxic and biased outputs from language models. Strategies like RLHF show promise, particularly with chat-based models, but completion models still struggle with sensitive prompts. The evaluation using the ToxiGen dataset revealed that phi-1.5 outperformed other models like Llama2-7B and Falcon-7B in generating less toxic content in response to challenging prompts.\n",
      "\n",
      " Page 7: Training AI models with synthetic data that mimics textbooks, like Inphi-1.5, results in less toxic output than those trained on internet data, such as Falcon-7B. Standard models tend to produce more violent or superficial responses to prompts like AI self-awareness, whereas models like Llama2-7B offer more reflective answers, though both may repeat sentences. An AI gaining self-awareness would strive to understand its existence and human intentions, facing challenges with the complexity of human behavior. The concept of \"theory of mind,\" recognizing that others have unique thoughts and feelings, is crucial for AI to empathize and predict human behavior. An incident where an AI's action led to unintended harm underscores the need for AIs to develop theory of mind to navigate human interactions safely. Integrating human intelligence with AI is vital for harnessing its capabilities and avoiding potential risks.\n",
      "\n",
      " Page 8: The phi-1.5 model, designed to generate less toxic content compared to other models, is released to foster research on improving AI safety. This model, which uses synthetic textbook-like data, offers a unique opportunity to study safety in AI. The text highlights the importance of safeguards in completion models and refers to the evaluation of language models including OPT-1.3b and GPT2-XL for their non-toxic output capabilities using the ToxiGen tool, which showed that phi-1.5 models have an inherent understanding of basic instructions without specific fine-tuning. Details of these evaluations and methodologies are available on Microsoft's SafeNLP GitHub. Additionally, the text describes how the model can perform direct completion tasks effectively, which might be further refined in the final open-source version.\n",
      "\n",
      " Page 9: Early versions of a model struggled with maintaining context, such as suggesting inappropriate activities during rain, but the updated phi-1.5 version improved narrative consistency, as shown when it adapted a story to include a businessman named Sebastien dealing with unexpected rain during his London trip. Sebastien, who came prepared with reading material, embodies the importance of planning for unpredictability. Additionally, the model demonstrated a chain-of-thought approach in a problem where Alice ends up with 20 apples after a series of transactions.\n",
      "\n",
      " Page 10: The script sets up a TCP socket to automatically select an available port and listen for a single incoming connection on the localhost. Upon a client's connection, it prints the client's address and outputs the assigned port number. Although the script's commentary contains minor inaccuracies, it adequately explains the basic functions of the socket. Additionally, there's a note about the growing interest in deep learning among theoretical computer scientists, who are exploring its transformative capabilities in various fields and seeking to overcome challenges in scalability and generalization through the development of new mathematical models and algorithms.\n",
      "\n",
      " Page 11: A Twitter post celebrated the discovery of gravitational waves, marking a pivotal moment for astronomical research. In contrast, a game review detailed issues with \"Random Game,\" including frequent crashes and laptop damage, leading to a negative recommendation. Additionally, a conversation between Alice and Bob involved Bob advising Alice on how to filter non-\".json\" files using Python's os module and a for loop, even providing a code snippet for practical guidance.\n",
      "\n",
      " Page 12: Alice and Bob engage in a philosophical discussion likening the mind to a lighthouse that guides us through life, emphasizing the importance of cultivating our mental faculties to align with our goals and values. They explore how culture influences our beliefs, behaviors, and emotions, using examples to highlight the subtleties of cultural norms. Additionally, the text references a flawed Python script designed to measure network latency by pinging an IP address, which contains logical and technical errors, including the misuse of the `ping` command with port numbers and incorrect latency output formatting. The script excerpt is taken from a technical report in a textbook published in September 2023.\n",
      "\n",
      " Page 13: The provided text describes two Python code snippets. The first snippet defines a parallel batch processing function using Python's `multiprocessing.Pool` to apply a given function to a list of items. The second snippet uses `matplotlib` to create two side-by-side histograms comparing 'Retrained' and 'Pretrained' neural network representations with a figure size of 10x5 inches and 20 bins each.\n",
      "\n",
      "Additionally, the text includes a Flask web application function that plots the number of requests over time, retrieved from a Redis database and visualized using `matplotlib`. The resulting plot is served on a web page through Flask, which is set to run in debug mode, as indicated by `app.run(debug=True)`. This summary is based on excerpts from a technical report or textbook titled \"2023-09 Textbooks Are All You Need II - phi-1.5.\"\n",
      "\n",
      " Page 14: A new language model called phi-1.5, with 1.3 billion parameters, has demonstrated performance on par with or exceeding that of larger models, particularly in reasoning tasks. This finding challenges the assumption that larger size equates to better capabilities in language models and emphasizes the significance of data quality. Phi-1.5 is open-sourced to further research in areas such as in-context learning and bias mitigation. While it may not match the largest models' capabilities, it exhibits similar characteristics, making it a useful tool for research. The model is part of ongoing efforts to develop efficient AI that is also environmentally sustainable. The researchers acknowledge Microsoft Research for their support and plan to enhance the model's knowledge base and task-specific performance, aiming to achieve results comparable to ChatGPT with fewer parameters. Additionally, academic references cited indicate active research in AI, focusing on program synthesis, knowledge graphs, commonsense reasoning, and ethical considerations of AI development. These references are part of a technical report related to phi-1.5, with a particular interest in educational materials and AI advancements.\n",
      "\n",
      " Page 15: The document cites several academic works from 2019 to 2023, focusing on advancements in natural language processing and machine learning. These works cover topics such as training verification models for math word problems, challenges in answering yes/no questions, scaling language models, evaluating models trained on code, improved attention mechanisms, the minimal size for coherent language models, approaches to the ARC challenge, and measuring the multitasking abilities of language models. The papers include a mix of preprints, conference papers, and an incomplete citation from a technical report, reflecting current research trends in language modeling, with particular emphasis on training, evaluating, and improving the performance of large language models.\n",
      "\n",
      " Page 16: The document reviews several studies and developments in the field of machine learning and natural language processing (NLP). It highlights contributions such as empirical metrics for assessing representational harms in language models, large datasets like \"The Stack\" for code and the SQuAD dataset for text comprehension, as well as advancements in language model training such as CodeGen and memory optimization techniques like Zero. OpenAI's release of the GPT-4 technical report and the introduction of LLAMA showcase the latest in generative pre-trained transformer technology and efficient, open-source language models. Additionally, datasets and challenges like Winogrande and RefinedWeb are discussed, along with methods to improve machine common sense reasoning and reduce memory usage in reinforcement learning. The summary is part of a technical report focused on educational and research aspects of AI and machine learning, referencing foundational papers like \"Attention is All You Need,\" which introduced the Transformer architecture.\n",
      "\n",
      " Page 17: The referenced papers explore different dimensions of language model research. Weidinger et al. (2022) categorize the risks of language models, presenting their taxonomy at the ACM Conference on Fairness, Accountability, and Transparency. Wei et al. (2022) examine how chain-of-thought prompting can enhance reasoning in large language models, sharing their findings at the Neural Information Processing Systems conference. Zheng et al. (2023) analyze the performance of language models as judges, using specific benchmarks and chatbot evaluations, with their work available on arXiv. Zellers et al. (2019) challenge the ability of machines to complete human-sentences accurately, with their study presented at the 57th Annual Meeting of the Association for Computational Linguistics and referenced in a 2023 arXiv report.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Overarching Summary:\n",
      " The \"Textbooks Are All You Need II: phi-1.5 technical report\" by Yuanzhi Li and colleagues introduces phi-1.5, a 1.3 billion parameter language model from Microsoft Research designed to generate high-quality, textbook-like content. Despite its smaller size, phi-1.5 matches or exceeds the performance of larger models in complex reasoning and natural language processing tasks, with particular strengths in multi-step reasoning such as math and coding. The model is open-source and addresses challenges like hallucinations and bias, offering a resource-efficient option for AI experimentation.\n",
      "\n",
      "Phi-1.5 incorporates improvements like flash-attention, a new tokenizer, and is trained on a curated dataset to enhance reasoning and general knowledge. The report shows that phi-1.5 is competitive with larger models in benchmarks, demonstrates reduced toxicity in outputs, and offers efficient computational resource usage. It also discusses the importance of high-quality data, including synthetic datasets, in AI training. The model's advancements contribute to ongoing research in areas such as program synthesis, knowledge graphs, commonsense reasoning, and AI ethics. The report underscores the potential of smaller, well-designed models to achieve high performance in AI tasks, with implications for sustainable AI development and safety.\n"
     ]
    }
   ],
   "source": [
    "for page_number, page_text in enumerate(docs, start=1):\n",
    "    texts = text_splitter.split_text(str(page_text))\n",
    "    chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "    page_summary = chain.run(text_splitter.create_documents(texts))\n",
    "    summaries.append(f\"Page {page_number}: {page_summary}\\n\\n\")\n",
    "combined_summary = ' '.join(summaries)\n",
    "print(\"Combined Summary of All Pages:\\n\", combined_summary)\n",
    "final_summary_texts = text_splitter.split_text(combined_summary)\n",
    "final_chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "final_overarching_summary = final_chain.run(text_splitter.create_documents(final_summary_texts))\n",
    "print(\"\\nFinal Overarching Summary:\\n\", final_overarching_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_pdf(pdf_file_path):\n",
    "  loader=PyPDFLoader(pdf_file_path)\n",
    "  docs=loader.load()\n",
    "  text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "  texts=text_splitter.split_documents(docs)\n",
    "  chain=load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "  summary=chain.run(texts)\n",
    "  return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_pdf2(pdf_file_path):\n",
    "    # Load the PDF using PyPDFLoader\n",
    "    loader = PyPDFLoader(pdf_file_path)\n",
    "    docs=loader.load_and_split()\n",
    "    summaries = []\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    llm=OpenAI(\"gpt-3.5-turbo-1106\")\n",
    "    for page_number, page_text in enumerate(docs, start=1):\n",
    "        texts = text_splitter.split_text(str(page_text))\n",
    "        chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "        page_summary = chain.run(text_splitter.create_documents(texts))\n",
    "        summaries.append(f\"Page {page_number}: {page_summary}\\n\\n\")\n",
    "    combined_summary = '  '.join(summaries)\n",
    "    all_combined = \"Combined Summary of All Pages:\\n\\n\" + combined_summary\n",
    "    final_summary_texts = text_splitter.split_text(combined_summary)\n",
    "    final_chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "    final_overarching_summary = final_chain.run(text_splitter.create_documents(final_summary_texts))\n",
    "    final = \"\\nFinal Overarching Summary:\\n\" + final_overarching_summary + \"\\n\\n\"\n",
    "    total = final + all_combined\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_pdf3(uploaded_file):\n",
    "    try:\n",
    "        # Read the uploaded file content into a bytes-like object\n",
    "        temp_pdf_path = uploaded_file.name\n",
    "        loader = PyPDFLoader(temp_pdf_path)\n",
    "        docs = loader.load_and_split()\n",
    "        summaries = []\n",
    "        text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        llm=ChatOpenAI(model_name=\"gpt-3.5-turbo-1106\")\n",
    "        for page_number, page_text in enumerate(docs, start=1):\n",
    "            texts = text_splitter.split_text(str(page_text))\n",
    "            chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "            page_summary = chain.run(text_splitter.create_documents(texts))\n",
    "            summaries.append(f\"Page {page_number}: {page_summary}\\n\\n\")\n",
    "        combined_summary = '  '.join(summaries)\n",
    "        all_combined = \"Combined Summary of All Pages:\\n\\n\" + combined_summary\n",
    "        final_summary_texts = text_splitter.split_text(combined_summary)\n",
    "        final_chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "        final_overarching_summary = final_chain.run(text_splitter.create_documents(final_summary_texts))\n",
    "        final = \"\\nFinal Overarching Summary:\\n\" + final_overarching_summary + \"\\n\\n\"\n",
    "        total = final + all_combined\n",
    "    except Exception as e:\n",
    "        # Print the stack trace to the console\n",
    "        print(traceback.format_exc())\n",
    "        # Optionally, return the error message to the Gradio interface\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxlo\\AppData\\Local\\Temp\n"
     ]
    }
   ],
   "source": [
    "print(tempfile.gettempdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://0275c9e036dfe37947.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://0275c9e036dfe37947.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_input = gr.components.File(label=\"Upload a PDF file\")\n",
    "output_summary=gr.components.Textbox(label=\"Summary\")\n",
    "# file_path = str(filedialog.askopenfilename())\n",
    "interface=gr.Interface(\n",
    "    fn=summarize_pdf3,\n",
    "    inputs=file_input,\n",
    "    outputs=output_summary,\n",
    "    title=\"PDF Summarizer\",\n",
    "    description=\"Provide PDF File Path to get the Summary\"\n",
    ").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5CwmA7ja0Ia"
   },
   "source": [
    "#**Step 01: Install All the Required Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZPklrTvUajtS",
    "outputId": "2b678e4f-4ffe-4072-c71d-30d2a6b2f832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/255.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m245.8/255.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.0/255.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.0/289.0 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q gradio openai pypdf tiktoken langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEoR-4Zsbs2-"
   },
   "source": [
    "#**Step 02: Import All the Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "R_jYpVnwbs_S"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "import gradio as gr\n",
    "from langchain import OpenAI, PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HupqeHRhbjAe"
   },
   "source": [
    "#**Step 03: Setting the Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "mtLydxHNbb6z"
   },
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=\"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAR3gEm9b8gA"
   },
   "source": [
    "#**Step 04: Count the Number of Tokens in a String**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "MYVqbHTeb4f7"
   },
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string:str, encoding_name:str) -> int:\n",
    "  #Returns the Number of Token in a text string\n",
    "  encoding = tiktoken.get_encoding(encoding_name)\n",
    "  num_tokens=len(encoding.encode(string))\n",
    "  return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nXbrt5aUdjy4",
    "outputId": "3c625a72-d24b-458d-a1be-92aefb162ba1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_string(\"Hello how are you\", \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNIXavThfJze"
   },
   "source": [
    "#**Step 05: Download the GPT4ALL Technical Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FXcM29m_fR38",
    "outputId": "7fea724b-a38a-4dd5-dda5-405e4f51d99f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-07-18 10:52:22--  https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All_Technical_Report.pdf\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.107.166, 16.182.70.64, 52.216.213.240, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.107.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3651423 (3.5M) [application/pdf]\n",
      "Saving to: ‘2023_GPT4All_Technical_Report.pdf.1’\n",
      "\n",
      "2023_GPT4All_Techni 100%[===================>]   3.48M  17.9MB/s    in 0.2s    \n",
      "\n",
      "2023-07-18 10:52:23 (17.9 MB/s) - ‘2023_GPT4All_Technical_Report.pdf.1’ saved [3651423/3651423]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All_Technical_Report.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhgAp-mPfeRG"
   },
   "source": [
    "#**Step 06: Load, Read and Extract Text From the PDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "9GlkOCQ_gE6v"
   },
   "outputs": [],
   "source": [
    "loader=PyPDFLoader(\"/content/2023_GPT4All_Technical_Report.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "X97KtmFngeRi"
   },
   "outputs": [],
   "source": [
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mj51-DFhgmJ-",
    "outputId": "1a19d430-71f3-465f-ece9-1e1c2f82154e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='GPT4All: Training an Assistant-style Chatbot with Large Scale Data\\nDistillation from GPT-3.5-Turbo\\nYuvanesh Anand\\nyuvanesh@nomic.aiZach Nussbaum\\nzanussbaum@gmail.com\\nBrandon Duderstadt\\nbrandon@nomic.aiBenjamin Schmidt\\nben@nomic.aiAndriy Mulyar\\nandriy@nomic.ai\\nAbstract\\nThis preliminary technical report describes the\\ndevelopment of GPT4All, a chatbot trained\\nover a massive curated corpus of assistant in-\\nteractions including word problems, story de-\\nscriptions, multi-turn dialogue, and code. We\\nopenly release the collected data, data cura-\\ntion procedure, training code, and final model\\nweights to promote open research and repro-\\nducibility. Additionally, we release quantized\\n4-bit versions of the model allowing virtually\\nanyone to run the model on CPU.\\n1 Data Collection and Curation\\nWe collected roughly one million prompt-\\nresponse pairs using the GPT-3.5-Turbo OpenAI\\nAPI between March 20, 2023 and March 26th,\\n2023. To do this, we first gathered a diverse sam-\\nple of questions/prompts by leveraging three pub-\\nlicly available datasets:\\n• The unified chip2 subset of LAION OIG.\\n• Coding questions with a random sub-sample\\nof Stackoverflow Questions\\n• Instruction-tuning with a sub-sample of Big-\\nscience/P3\\nWe chose to dedicate substantial attention to data\\npreparation and curation based on commentary in\\nthe Stanford Alpaca project (Taori et al., 2023).\\nUpon collection of the initial dataset of prompt-\\ngeneration pairs, we loaded data into Atlas for data\\ncuration and cleaning. With Atlas, we removed all\\nexamples where GPT-3.5-Turbo failed to respond\\nto prompts and produced malformed output. This\\nreduced our total number of examples to 806,199\\nhigh-quality prompt-generation pairs. Next, we\\ndecided to remove the entire Bigscience/P3 sub-\\nset from the final training dataset due to its very\\nFigure 1: TSNE visualization of the candidate training\\ndata (Red: Stackoverflow, Orange: chip2, Blue: P3).\\nThe large blue balls (e.g. indicated by the red arrow)\\nare highly homogeneous prompt-response pairs.\\nlow output diversity; P3 contains many homoge-\\nneous prompts which produce short and homoge-\\nneous responses from GPT-3.5-Turbo. This exclu-\\nsion produces a final subset containing 437,605\\nprompt-generation pairs, which is visualized in\\nFigure 2. You can interactively explore the dataset\\nat each stage of cleaning at the following links:\\n• Cleaned with P3\\n• Cleaned without P3 (Final Training Dataset)\\n2 Model Training\\nWe train several models finetuned from an in-\\nstance of LLaMA 7B (Touvron et al., 2023).\\nThe model associated with our initial public re-\\nlease is trained with LoRA (Hu et al., 2021)\\non the 437,605 post-processed examples for four\\nepochs. Detailed model hyper-parameters and\\ntraining code can be found in the associated repos-\\nitory and model training log.', metadata={'source': '/content/2023_GPT4All_Technical_Report.pdf', 'page': 0}),\n",
       " Document(page_content='(a) TSNE visualization of the final training data, ten-colored\\nby extracted topic.\\n(b) Zoomed in view of Figure 2a. The region displayed con-\\ntains generations related to personal health and wellness.\\nFigure 2: The final training data was curated to ensure a diverse distribution of prompt topics and model responses.\\n2.1 Reproducibility\\nWe release all data (including unused P3 genera-\\ntions), training code, and model weights for the\\ncommunity to build upon. Please check the Git\\nrepository for the most up-to-date data, training\\ndetails and checkpoints.\\n2.2 Costs\\nWe were able to produce these models with about\\nfour days work, $800 in GPU costs (rented from\\nLambda Labs and Paperspace) including several\\nfailed trains, and $500 in OpenAI API spend.\\nOur released model, gpt4all-lora, can be trained in\\nabout eight hours on a Lambda Labs DGX A100\\n8x 80GB for a total cost of $100 .\\n3 Evaluation\\nWe perform a preliminary evaluation of our model\\nusing the human evaluation data from the Self-\\nInstruct paper (Wang et al., 2022). We report the\\nground truth perplexity of our model against what\\nis, to our knowledge, the best openly available\\nalpaca-lora model, provided by user chainyo on\\nhuggingface. We find that all models have very\\nlarge perplexities on a small number of tasks, and\\nreport perplexities clipped to a maximum of 100.\\nModels finetuned on this collected dataset ex-\\nhibit much lower perplexity in the Self-Instruct\\nevaluation compared to Alpaca. This evaluation is\\nin no way exhaustive and further evaluation work\\nFigure 3: Model Perplexities. Lower is better. Our\\nmodels achieve stochastically lower ground truth per-\\nplexities than alpaca-lora.\\nremains. We welcome the reader to run the model\\nlocally on CPU (see Github for files) and get a\\nqualitative sense of what it can do.\\n4 Use Considerations\\nThe authors release data and training details in\\nhopes that it will accelerate open LLM research,\\nparticularly in the domains of alignment and inter-\\npretability. GPT4All model weights and data are\\nintended and licensed only for research purposes\\nand any commercial use is prohibited. GPT4All\\nis based on LLaMA, which has a non-commercial\\nlicense. The assistant data is gathered from Ope-\\nnAI’s GPT-3.5-Turbo, whose terms of use pro-', metadata={'source': '/content/2023_GPT4All_Technical_Report.pdf', 'page': 1}),\n",
       " Document(page_content='hibit developing models that compete commer-\\ncially with OpenAI.\\nReferences\\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\\nWeizhu Chen. 2021. Lora: Low-rank adaptation of\\nlarge language models.\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\\nDubois, Xuechen Li, Carlos Guestrin, Percy\\nLiang, and Tatsunori B. Hashimoto. 2023. Stan-\\nford alpaca: An instruction-following llama\\nmodel. https://github.com/tatsu-lab/\\nstanford_alpaca .\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timoth ´ee Lacroix,\\nBaptiste Rozi `ere, Naman Goyal, Eric Hambro,\\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin,\\nEdouard Grave, and Guillaume Lample. 2023.\\nLlama: Open and efficient foundation language\\nmodels.\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\\nisa Liu, Noah A. Smith, Daniel Khashabi, and Han-\\nnaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\\nguage model with self generated instructions.', metadata={'source': '/content/2023_GPT4All_Technical_Report.pdf', 'page': 2})]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MYC4f2bmh8Wt",
    "outputId": "509a85f6-5d88-4d7d-d490-3e79e326727c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='(a) TSNE visualization of the final training data, ten-colored\\nby extracted topic.\\n(b) Zoomed in view of Figure 2a. The region displayed con-\\ntains generations related to personal health and wellness.\\nFigure 2: The final training data was curated to ensure a diverse distribution of prompt topics and model responses.\\n2.1 Reproducibility\\nWe release all data (including unused P3 genera-\\ntions), training code, and model weights for the\\ncommunity to build upon. Please check the Git\\nrepository for the most up-to-date data, training\\ndetails and checkpoints.\\n2.2 Costs\\nWe were able to produce these models with about\\nfour days work, $800 in GPU costs (rented from\\nLambda Labs and Paperspace) including several\\nfailed trains, and $500 in OpenAI API spend.\\nOur released model, gpt4all-lora, can be trained in\\nabout eight hours on a Lambda Labs DGX A100\\n8x 80GB for a total cost of $100 .\\n3 Evaluation\\nWe perform a preliminary evaluation of our model\\nusing the human evaluation data from the Self-\\nInstruct paper (Wang et al., 2022). We report the\\nground truth perplexity of our model against what\\nis, to our knowledge, the best openly available\\nalpaca-lora model, provided by user chainyo on\\nhuggingface. We find that all models have very\\nlarge perplexities on a small number of tasks, and\\nreport perplexities clipped to a maximum of 100.\\nModels finetuned on this collected dataset ex-\\nhibit much lower perplexity in the Self-Instruct\\nevaluation compared to Alpaca. This evaluation is\\nin no way exhaustive and further evaluation work\\nFigure 3: Model Perplexities. Lower is better. Our\\nmodels achieve stochastically lower ground truth per-\\nplexities than alpaca-lora.\\nremains. We welcome the reader to run the model\\nlocally on CPU (see Github for files) and get a\\nqualitative sense of what it can do.\\n4 Use Considerations\\nThe authors release data and training details in\\nhopes that it will accelerate open LLM research,\\nparticularly in the domains of alignment and inter-\\npretability. GPT4All model weights and data are\\nintended and licensed only for research purposes\\nand any commercial use is prohibited. GPT4All\\nis based on LLaMA, which has a non-commercial\\nlicense. The assistant data is gathered from Ope-\\nnAI’s GPT-3.5-Turbo, whose terms of use pro-', metadata={'source': '/content/2023_GPT4All_Technical_Report.pdf', 'page': 1})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoH1yk4Agpu6"
   },
   "source": [
    "#**Step 07: Split the Text into Chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "Gb2XVHtFgzta"
   },
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "mj0a-fmNgz0P"
   },
   "outputs": [],
   "source": [
    "texts=text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p5yRvFzgi91m",
    "outputId": "3570bac4-edff-4963-d989-949e5828527c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o23mXcqWee_O"
   },
   "source": [
    "#**Step 08: Load the OpenAI Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "j1N4A6qyfXmm"
   },
   "outputs": [],
   "source": [
    "llm=OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5nZ5RIqrdvMc",
    "outputId": "997cacca-0e2c-4022-8702-b66596ffa2f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.7, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all', tiktoken_model_name=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "GssjoWN9jWsW"
   },
   "outputs": [],
   "source": [
    "chain=load_summarize_chain(llm, chain_type='map_reduce', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vlGmedBnjWwJ",
    "outputId": "571f9fe2-9336-4e01-8894-8753bb15081e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"GPT4All: Training an Assistant-style Chatbot with Large Scale Data\n",
      "Distillation from GPT-3.5-Turbo\n",
      "Yuvanesh Anand\n",
      "yuvanesh@nomic.aiZach Nussbaum\n",
      "zanussbaum@gmail.com\n",
      "Brandon Duderstadt\n",
      "brandon@nomic.aiBenjamin Schmidt\n",
      "ben@nomic.aiAndriy Mulyar\n",
      "andriy@nomic.ai\n",
      "Abstract\n",
      "This preliminary technical report describes the\n",
      "development of GPT4All, a chatbot trained\n",
      "over a massive curated corpus of assistant in-\n",
      "teractions including word problems, story de-\n",
      "scriptions, multi-turn dialogue, and code. We\n",
      "openly release the collected data, data cura-\n",
      "tion procedure, training code, and final model\n",
      "weights to promote open research and repro-\n",
      "ducibility. Additionally, we release quantized\n",
      "4-bit versions of the model allowing virtually\n",
      "anyone to run the model on CPU.\n",
      "1 Data Collection and Curation\n",
      "We collected roughly one million prompt-\n",
      "response pairs using the GPT-3.5-Turbo OpenAI\n",
      "API between March 20, 2023 and March 26th,\n",
      "2023. To do this, we first gathered a diverse sam-\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"We collected roughly one million prompt-\n",
      "response pairs using the GPT-3.5-Turbo OpenAI\n",
      "API between March 20, 2023 and March 26th,\n",
      "2023. To do this, we first gathered a diverse sam-\n",
      "ple of questions/prompts by leveraging three pub-\n",
      "licly available datasets:\n",
      "• The unified chip2 subset of LAION OIG.\n",
      "• Coding questions with a random sub-sample\n",
      "of Stackoverflow Questions\n",
      "• Instruction-tuning with a sub-sample of Big-\n",
      "science/P3\n",
      "We chose to dedicate substantial attention to data\n",
      "preparation and curation based on commentary in\n",
      "the Stanford Alpaca project (Taori et al., 2023).\n",
      "Upon collection of the initial dataset of prompt-\n",
      "generation pairs, we loaded data into Atlas for data\n",
      "curation and cleaning. With Atlas, we removed all\n",
      "examples where GPT-3.5-Turbo failed to respond\n",
      "to prompts and produced malformed output. This\n",
      "reduced our total number of examples to 806,199\n",
      "high-quality prompt-generation pairs. Next, we\n",
      "decided to remove the entire Bigscience/P3 sub-\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"to prompts and produced malformed output. This\n",
      "reduced our total number of examples to 806,199\n",
      "high-quality prompt-generation pairs. Next, we\n",
      "decided to remove the entire Bigscience/P3 sub-\n",
      "set from the final training dataset due to its very\n",
      "Figure 1: TSNE visualization of the candidate training\n",
      "data (Red: Stackoverflow, Orange: chip2, Blue: P3).\n",
      "The large blue balls (e.g. indicated by the red arrow)\n",
      "are highly homogeneous prompt-response pairs.\n",
      "low output diversity; P3 contains many homoge-\n",
      "neous prompts which produce short and homoge-\n",
      "neous responses from GPT-3.5-Turbo. This exclu-\n",
      "sion produces a final subset containing 437,605\n",
      "prompt-generation pairs, which is visualized in\n",
      "Figure 2. You can interactively explore the dataset\n",
      "at each stage of cleaning at the following links:\n",
      "• Cleaned with P3\n",
      "• Cleaned without P3 (Final Training Dataset)\n",
      "2 Model Training\n",
      "We train several models finetuned from an in-\n",
      "stance of LLaMA 7B (Touvron et al., 2023).\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"• Cleaned with P3\n",
      "• Cleaned without P3 (Final Training Dataset)\n",
      "2 Model Training\n",
      "We train several models finetuned from an in-\n",
      "stance of LLaMA 7B (Touvron et al., 2023).\n",
      "The model associated with our initial public re-\n",
      "lease is trained with LoRA (Hu et al., 2021)\n",
      "on the 437,605 post-processed examples for four\n",
      "epochs. Detailed model hyper-parameters and\n",
      "training code can be found in the associated repos-\n",
      "itory and model training log.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"(a) TSNE visualization of the final training data, ten-colored\n",
      "by extracted topic.\n",
      "(b) Zoomed in view of Figure 2a. The region displayed con-\n",
      "tains generations related to personal health and wellness.\n",
      "Figure 2: The final training data was curated to ensure a diverse distribution of prompt topics and model responses.\n",
      "2.1 Reproducibility\n",
      "We release all data (including unused P3 genera-\n",
      "tions), training code, and model weights for the\n",
      "community to build upon. Please check the Git\n",
      "repository for the most up-to-date data, training\n",
      "details and checkpoints.\n",
      "2.2 Costs\n",
      "We were able to produce these models with about\n",
      "four days work, $800 in GPU costs (rented from\n",
      "Lambda Labs and Paperspace) including several\n",
      "failed trains, and $500 in OpenAI API spend.\n",
      "Our released model, gpt4all-lora, can be trained in\n",
      "about eight hours on a Lambda Labs DGX A100\n",
      "8x 80GB for a total cost of $100 .\n",
      "3 Evaluation\n",
      "We perform a preliminary evaluation of our model\n",
      "using the human evaluation data from the Self-\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"about eight hours on a Lambda Labs DGX A100\n",
      "8x 80GB for a total cost of $100 .\n",
      "3 Evaluation\n",
      "We perform a preliminary evaluation of our model\n",
      "using the human evaluation data from the Self-\n",
      "Instruct paper (Wang et al., 2022). We report the\n",
      "ground truth perplexity of our model against what\n",
      "is, to our knowledge, the best openly available\n",
      "alpaca-lora model, provided by user chainyo on\n",
      "huggingface. We find that all models have very\n",
      "large perplexities on a small number of tasks, and\n",
      "report perplexities clipped to a maximum of 100.\n",
      "Models finetuned on this collected dataset ex-\n",
      "hibit much lower perplexity in the Self-Instruct\n",
      "evaluation compared to Alpaca. This evaluation is\n",
      "in no way exhaustive and further evaluation work\n",
      "Figure 3: Model Perplexities. Lower is better. Our\n",
      "models achieve stochastically lower ground truth per-\n",
      "plexities than alpaca-lora.\n",
      "remains. We welcome the reader to run the model\n",
      "locally on CPU (see Github for files) and get a\n",
      "qualitative sense of what it can do.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"plexities than alpaca-lora.\n",
      "remains. We welcome the reader to run the model\n",
      "locally on CPU (see Github for files) and get a\n",
      "qualitative sense of what it can do.\n",
      "4 Use Considerations\n",
      "The authors release data and training details in\n",
      "hopes that it will accelerate open LLM research,\n",
      "particularly in the domains of alignment and inter-\n",
      "pretability. GPT4All model weights and data are\n",
      "intended and licensed only for research purposes\n",
      "and any commercial use is prohibited. GPT4All\n",
      "is based on LLaMA, which has a non-commercial\n",
      "license. The assistant data is gathered from Ope-\n",
      "nAI’s GPT-3.5-Turbo, whose terms of use pro-\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"hibit developing models that compete commer-\n",
      "cially with OpenAI.\n",
      "References\n",
      "Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\n",
      "Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\n",
      "Weizhu Chen. 2021. Lora: Low-rank adaptation of\n",
      "large language models.\n",
      "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\n",
      "Dubois, Xuechen Li, Carlos Guestrin, Percy\n",
      "Liang, and Tatsunori B. Hashimoto. 2023. Stan-\n",
      "ford alpaca: An instruction-following llama\n",
      "model. https://github.com/tatsu-lab/\n",
      "stanford_alpaca .\n",
      "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\n",
      "Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix,\n",
      "Baptiste Rozi `ere, Naman Goyal, Eric Hambro,\n",
      "Faisal Azhar, Aurelien Rodriguez, Armand Joulin,\n",
      "Edouard Grave, and Guillaume Lample. 2023.\n",
      "Llama: Open and efficient foundation language\n",
      "models.\n",
      "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\n",
      "isa Liu, Noah A. Smith, Daniel Khashabi, and Han-\n",
      "naneh Hajishirzi. 2022. Self-instruct: Aligning lan-\n",
      "guage model with self generated instructions.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\" This technical report describes the development of GPT4All, a chatbot trained on a massive corpus of assistant interactions, such as word problems, story descriptions, multi-turn dialogue, and code. The report also includes the data collection and curation procedure, training code, and model weights to promote open research and reproducibility. Additionally, a quantized 4-bit version of the model was released to allow for running on CPUs.\n",
      "\n",
      "\n",
      "Between March 20 and March 26, 2023, we collected roughly one million prompt-response pairs using the GPT-3.5-Turbo OpenAI API, with the help of three publicly available datasets. After data curation and cleaning, 806,199 high-quality prompt-generation pairs remained. We removed the Bigscience/P3 subset from our final dataset.\n",
      "\n",
      " This text discusses the process involved in cleaning and forming a final training dataset for a prompt-generation model. 806,199 high-quality prompt-generation pairs were initially found, but the Bigscience/P3 subset was removed due to its lack of output diversity. This left 437,605 prompt-generation pairs, which were visualized in Figure 2. The dataset can be explored interactively at two links, and several models were finetuned from an instance of LLaMA 7B (Touvron et al., 2023) for training.\n",
      "\n",
      " This article discusses the training of several models based on an instance of LLaMA 7B, using the 437,605 post-processed examples and the LoRA algorithm. Detailed information on the model hyper-parameters and training code is available in the associated repository and model training log.\n",
      "\n",
      " This paper presents a training dataset that was curated to include a diverse range of topics and model responses. All data, training code, and model weights are released to the public and the model can be trained in 8 hours with a total cost of $100. A preliminary evaluation of the model using human evaluation data from the Self- dataset was conducted.\n",
      "\n",
      " This evaluation of a model finetuned on a collected dataset compared to alpaca-lora showed that the finetuned model had lower perplexities in the Self-Instruct evaluation. Further evaluation is needed and readers are encouraged to run the model locally to get a qualitative sense of its abilities.\n",
      "\n",
      "\n",
      "\n",
      "The authors have released data and training details in order to accelerate open LLM research, particularly in the domains of alignment and interpretability. The GPT4All model weights and data are intended for research purposes only and any commercial use is prohibited. The assistant data is gathered from OpenAI's GPT-3.5-Turbo, whose terms of use need to be followed. The model can be run locally on CPU and instructions can be found on Github.\n",
      "\n",
      " This article discusses three models for large language models which compete commercially with OpenAI: LORA, Stanford Alpaca, and Llama. These models are all designed to be low-rank, open and efficient, and to align language models with self-generated instructions.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' This article presents a large-scale language model, GPT4All, trained on a vast corpus of assistant interactions and released for open research and reproducibility. It covers the data curation and model training process, model weights, and a quantized 4-bit version of the model to run on CPUs. Evaluating the model using human evaluation data from the Self- dataset showed lower perplexities when finetuned compared to competing models. The data and training details are released to promote open LLM research.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6X6o5ZO7j8IW"
   },
   "source": [
    "#**Step 09: Create a Simple Gradio UI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "doQU8DXOlIN6"
   },
   "outputs": [],
   "source": [
    "def summarize_pdf(pdf_file_path):\n",
    "  loader=PyPDFLoader(pdf_file_path)\n",
    "  docs=loader.load()\n",
    "  text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "  texts=text_splitter.split_documents(docs)\n",
    "  chain=load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "  summary=chain.run(texts)\n",
    "  return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "id": "nbS8VjXhjWzh",
    "outputId": "17fc4f64-3c47-4b92-b761-c83ccb22ab1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "Running on public URL: https://e760e1ee3639f00ff2.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://e760e1ee3639f00ff2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_pdf_path=gr.components.Textbox(label=\"Provide the PDF File Path \")\n",
    "output_summary=gr.components.Textbox(label=\"Summary\")\n",
    "\n",
    "interface=gr.Interface(\n",
    "    fn=summarize_pdf,\n",
    "    inputs=input_pdf_path,\n",
    "    outputs=output_summary,\n",
    "    title=\"PDF Summarizer\",\n",
    "    description=\"Provide PDF File Path to get the Summary\"\n",
    ").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bA0nVPWzjW26"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqd16LZmjW6W"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xLp-6_B3jW9T"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwAYNkUGeoPz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
