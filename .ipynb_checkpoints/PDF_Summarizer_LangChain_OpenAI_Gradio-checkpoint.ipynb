{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import tiktoken\n",
    "import gradio as gr\n",
    "from langchain import OpenAI, PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  1 3565k    1 50636    0     0  30524      0  0:01:59  0:00:01  0:01:58 30540\n",
      " 31 3565k   31 1120k    0     0   414k      0  0:00:08  0:00:02  0:00:06  414k\n",
      " 62 3565k   62 2242k    0     0   589k      0  0:00:06  0:00:03  0:00:03  589k\n",
      " 96 3565k   96 3443k    0     0   727k      0  0:00:04  0:00:04 --:--:--  727k\n",
      "100 3565k  100 3565k    0     0   752k      0  0:00:04  0:00:04 --:--:--  911k\n"
     ]
    }
   ],
   "source": [
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"OPENAI_API_KEY\"]=openai_api_key\n",
    "def num_tokens_from_string(string:str, encoding_name:str) -> int:\n",
    "  #Returns the Number of Token in a text string\n",
    "  encoding = tiktoken.get_encoding(encoding_name)\n",
    "  num_tokens=len(encoding.encode(string))\n",
    "  return num_tokens\n",
    "!curl https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All_Technical_Report.pdf -o 2023_GPT4All_Technical_Report.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"GPT4All: Training an Assistant-style Chatbot with Large Scale Data\n",
      "Distillation from GPT-3.5-Turbo\n",
      "Yuvanesh Anand\n",
      "yuvanesh@nomic.aiZach Nussbaum\n",
      "zanussbaum@gmail.com\n",
      "Brandon Duderstadt\n",
      "brandon@nomic.aiBenjamin Schmidt\n",
      "ben@nomic.aiAndriy Mulyar\n",
      "andriy@nomic.ai\n",
      "Abstract\n",
      "This preliminary technical report describes the\n",
      "development of GPT4All, a chatbot trained\n",
      "over a massive curated corpus of assistant in-\n",
      "teractions including word problems, story de-\n",
      "scriptions, multi-turn dialogue, and code. We\n",
      "openly release the collected data, data cura-\n",
      "tion procedure, training code, and final model\n",
      "weights to promote open research and repro-\n",
      "ducibility. Additionally, we release quantized\n",
      "4-bit versions of the model allowing virtually\n",
      "anyone to run the model on CPU.\n",
      "1 Data Collection and Curation\n",
      "We collected roughly one million prompt-\n",
      "response pairs using the GPT-3.5-Turbo OpenAI\n",
      "API between March 20, 2023 and March 26th,\n",
      "2023. To do this, we first gathered a diverse sam-\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"We collected roughly one million prompt-\n",
      "response pairs using the GPT-3.5-Turbo OpenAI\n",
      "API between March 20, 2023 and March 26th,\n",
      "2023. To do this, we first gathered a diverse sam-\n",
      "ple of questions/prompts by leveraging three pub-\n",
      "licly available datasets:\n",
      "• The unified chip2 subset of LAION OIG.\n",
      "• Coding questions with a random sub-sample\n",
      "of Stackoverflow Questions\n",
      "• Instruction-tuning with a sub-sample of Big-\n",
      "science/P3\n",
      "We chose to dedicate substantial attention to data\n",
      "preparation and curation based on commentary in\n",
      "the Stanford Alpaca project (Taori et al., 2023).\n",
      "Upon collection of the initial dataset of prompt-\n",
      "generation pairs, we loaded data into Atlas for data\n",
      "curation and cleaning. With Atlas, we removed all\n",
      "examples where GPT-3.5-Turbo failed to respond\n",
      "to prompts and produced malformed output. This\n",
      "reduced our total number of examples to 806,199\n",
      "high-quality prompt-generation pairs. Next, we\n",
      "decided to remove the entire Bigscience/P3 sub-\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"to prompts and produced malformed output. This\n",
      "reduced our total number of examples to 806,199\n",
      "high-quality prompt-generation pairs. Next, we\n",
      "decided to remove the entire Bigscience/P3 sub-\n",
      "set from the final training dataset due to its very\n",
      "Figure 1: TSNE visualization of the candidate training\n",
      "data (Red: Stackoverflow, Orange: chip2, Blue: P3).\n",
      "The large blue balls (e.g. indicated by the red arrow)\n",
      "are highly homogeneous prompt-response pairs.\n",
      "low output diversity; P3 contains many homoge-\n",
      "neous prompts which produce short and homoge-\n",
      "neous responses from GPT-3.5-Turbo. This exclu-\n",
      "sion produces a final subset containing 437,605\n",
      "prompt-generation pairs, which is visualized in\n",
      "Figure 2. You can interactively explore the dataset\n",
      "at each stage of cleaning at the following links:\n",
      "• Cleaned with P3\n",
      "• Cleaned without P3 (Final Training Dataset)\n",
      "2 Model Training\n",
      "We train several models finetuned from an in-\n",
      "stance of LLaMA 7B (Touvron et al., 2023).\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"• Cleaned with P3\n",
      "• Cleaned without P3 (Final Training Dataset)\n",
      "2 Model Training\n",
      "We train several models finetuned from an in-\n",
      "stance of LLaMA 7B (Touvron et al., 2023).\n",
      "The model associated with our initial public re-\n",
      "lease is trained with LoRA (Hu et al., 2021)\n",
      "on the 437,605 post-processed examples for four\n",
      "epochs. Detailed model hyper-parameters and\n",
      "training code can be found in the associated repos-\n",
      "itory and model training log.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"(a) TSNE visualization of the final training data, ten-colored\n",
      "by extracted topic.\n",
      "(b) Zoomed in view of Figure 2a. The region displayed con-\n",
      "tains generations related to personal health and wellness.\n",
      "Figure 2: The final training data was curated to ensure a diverse distribution of prompt topics and model responses.\n",
      "2.1 Reproducibility\n",
      "We release all data (including unused P3 genera-\n",
      "tions), training code, and model weights for the\n",
      "community to build upon. Please check the Git\n",
      "repository for the most up-to-date data, training\n",
      "details and checkpoints.\n",
      "2.2 Costs\n",
      "We were able to produce these models with about\n",
      "four days work, $800 in GPU costs (rented from\n",
      "Lambda Labs and Paperspace) including several\n",
      "failed trains, and $500 in OpenAI API spend.\n",
      "Our released model, gpt4all-lora, can be trained in\n",
      "about eight hours on a Lambda Labs DGX A100\n",
      "8x 80GB for a total cost of $100 .\n",
      "3 Evaluation\n",
      "We perform a preliminary evaluation of our model\n",
      "using the human evaluation data from the Self-\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"about eight hours on a Lambda Labs DGX A100\n",
      "8x 80GB for a total cost of $100 .\n",
      "3 Evaluation\n",
      "We perform a preliminary evaluation of our model\n",
      "using the human evaluation data from the Self-\n",
      "Instruct paper (Wang et al., 2022). We report the\n",
      "ground truth perplexity of our model against what\n",
      "is, to our knowledge, the best openly available\n",
      "alpaca-lora model, provided by user chainyo on\n",
      "huggingface. We find that all models have very\n",
      "large perplexities on a small number of tasks, and\n",
      "report perplexities clipped to a maximum of 100.\n",
      "Models finetuned on this collected dataset ex-\n",
      "hibit much lower perplexity in the Self-Instruct\n",
      "evaluation compared to Alpaca. This evaluation is\n",
      "in no way exhaustive and further evaluation work\n",
      "Figure 3: Model Perplexities. Lower is better. Our\n",
      "models achieve stochastically lower ground truth per-\n",
      "plexities than alpaca-lora.\n",
      "remains. We welcome the reader to run the model\n",
      "locally on CPU (see Github for files) and get a\n",
      "qualitative sense of what it can do.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"plexities than alpaca-lora.\n",
      "remains. We welcome the reader to run the model\n",
      "locally on CPU (see Github for files) and get a\n",
      "qualitative sense of what it can do.\n",
      "4 Use Considerations\n",
      "The authors release data and training details in\n",
      "hopes that it will accelerate open LLM research,\n",
      "particularly in the domains of alignment and inter-\n",
      "pretability. GPT4All model weights and data are\n",
      "intended and licensed only for research purposes\n",
      "and any commercial use is prohibited. GPT4All\n",
      "is based on LLaMA, which has a non-commercial\n",
      "license. The assistant data is gathered from Ope-\n",
      "nAI’s GPT-3.5-Turbo, whose terms of use pro-\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"hibit developing models that compete commer-\n",
      "cially with OpenAI.\n",
      "References\n",
      "Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\n",
      "Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\n",
      "Weizhu Chen. 2021. Lora: Low-rank adaptation of\n",
      "large language models.\n",
      "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\n",
      "Dubois, Xuechen Li, Carlos Guestrin, Percy\n",
      "Liang, and Tatsunori B. Hashimoto. 2023. Stan-\n",
      "ford alpaca: An instruction-following llama\n",
      "model. https://github.com/tatsu-lab/\n",
      "stanford_alpaca .\n",
      "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\n",
      "Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix,\n",
      "Baptiste Rozi `ere, Naman Goyal, Eric Hambro,\n",
      "Faisal Azhar, Aurelien Rodriguez, Armand Joulin,\n",
      "Edouard Grave, and Guillaume Lample. 2023.\n",
      "Llama: Open and efficient foundation language\n",
      "models.\n",
      "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\n",
      "isa Liu, Noah A. Smith, Daniel Khashabi, and Han-\n",
      "naneh Hajishirzi. 2022. Self-instruct: Aligning lan-\n",
      "guage model with self generated instructions.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\" \n",
      "This paper describes the development of GPT4All, a chatbot trained on a large dataset of assistant interactions such as word problems, stories, dialogue, and code. The authors release the data, data curation procedure, training code, and model weights for open research and reproducibility. Additionally, they release a quantized version of the model for anyone to run on CPU.\n",
      "\n",
      " This project collected one million prompt-response pairs using the GPT-3.5-Turbo OpenAI API between March 20-26, 2023. Data was gathered from three publicly available datasets and data preparation and curation was based on the Stanford Alpaca project (Taori et al., 2023). After data curation and cleaning with Atlas, the number of examples was reduced to 806,199 high-quality prompt-generation pairs. The entire Bigscience/P3 subset was removed.\n",
      "\n",
      " This article describes the process of cleaning a training dataset of prompt-generation pairs and training several models from an instance of LLaMA 7B. After prompt cleaning, the number of examples was reduced to 806,199 and the Bigscience/P3 subset was removed due to low output diversity, resulting in a final subset of 437,605. Interactively explorations of the dataset can be accessed at two links.\n",
      "\n",
      "\n",
      "This report outlines the training of several models finetuned from an instance of LLaMA 7B (Touvron et al., 2023). The model associated with the initial public release was trained with LoRA (Hu et al., 2021) on 437,605 post-processed examples for four epochs. Detailed model hyper-parameters and training code can be found in the associated repository and model training log.\n",
      "\n",
      " This paper presents a new model, gpt4all-lora, which was trained on a diverse set of prompts and model responses. It was produced with 4 days of work, $800 in GPU costs, and $500 in OpenAI API spend. The model was evaluated using human evaluation data from the Self-Reflect dataset. All data, training code, and model weights have been released so the community can build on it.\n",
      "\n",
      " We performed a preliminary evaluation of our model using data from the Self-Instruct paper, finding that our model had much lower perplexity than the best available alpaca-lora model. We encourage readers to run the model locally on CPU and get a qualitative sense of its performance.\n",
      "\n",
      " The authors present GPT4All, a model based on LLaMA and OpenAI's GPT-3.5-Turbo, that can accelerate open language learning model (LLM) research in the domains of alignment and interpretability. The model and data are released for research purposes only, and any commercial use is prohibited.\n",
      "\n",
      " This article discusses the development of language models that are commercially competitive with OpenAI. It references three models - LORA, Stanford Alpaca, and LLAMA - which are all open and efficient foundations for language models. Lastly, it discusses Self-Instruct, which is a language model designed to align with self-generated instructions.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' This paper presents GPT4All, a chatbot trained on a large dataset of assistant interactions that is released for open research and reproducibility. The data was gathered from three publicly available datasets, and the model was trained with LoRA. Additionally, the authors discuss the development of language models that are commercially competitive with OpenAI and present three models - LORA, Stanford Alpaca, and LLAMA - which are all open and efficient foundations for language models.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=PyPDFLoader(\"2023_GPT4All_Technical_Report.pdf\")\n",
    "docs=loader.load()\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts=text_splitter.split_documents(docs)\n",
    "llm=OpenAI()\n",
    "chain=load_summarize_chain(llm, chain_type='map_reduce', verbose=True)\n",
    "chain.run(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_pdf(pdf_file_path):\n",
    "  loader=PyPDFLoader(pdf_file_path)\n",
    "  docs=loader.load()\n",
    "  text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "  texts=text_splitter.split_documents(docs)\n",
    "  chain=load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "  summary=chain.run(texts)\n",
    "  return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://a9c118882735c705a6.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a9c118882735c705a6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxlo\\anaconda3\\Lib\\site-packages\\gradio\\queueing.py\", line 459, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxlo\\anaconda3\\Lib\\site-packages\\gradio\\route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxlo\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1533, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxlo\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1151, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxlo\\anaconda3\\Lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxlo\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxlo\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxlo\\anaconda3\\Lib\\site-packages\\gradio\\utils.py\", line 678, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxlo\\AppData\\Local\\Temp\\ipykernel_13528\\673775338.py\", line 2, in summarize_pdf\n",
      "    loader=PyPDFLoader(pdf_file_path)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxlo\\anaconda3\\Lib\\site-packages\\langchain_community\\document_loaders\\pdf.py\", line 157, in __init__\n",
      "    super().__init__(file_path, headers=headers)\n",
      "  File \"C:\\Users\\maxlo\\anaconda3\\Lib\\site-packages\\langchain_community\\document_loaders\\pdf.py\", line 100, in __init__\n",
      "    raise ValueError(\"File path %s is not a valid file or url\" % self.file_path)\n",
      "ValueError: File path 2023_GPT4All_Technical_Report is not a valid file or url\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxlo\\anaconda3\\Lib\\site-packages\\gradio\\queueing.py\", line 459, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxlo\\anaconda3\\Lib\\site-packages\\gradio\\route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxlo\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1533, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxlo\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1151, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxlo\\anaconda3\\Lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxlo\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxlo\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxlo\\anaconda3\\Lib\\site-packages\\gradio\\utils.py\", line 678, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxlo\\AppData\\Local\\Temp\\ipykernel_13528\\673775338.py\", line 2, in summarize_pdf\n",
      "    loader=PyPDFLoader(pdf_file_path)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxlo\\anaconda3\\Lib\\site-packages\\langchain_community\\document_loaders\\pdf.py\", line 157, in __init__\n",
      "    super().__init__(file_path, headers=headers)\n",
      "  File \"C:\\Users\\maxlo\\anaconda3\\Lib\\site-packages\\langchain_community\\document_loaders\\pdf.py\", line 100, in __init__\n",
      "    raise ValueError(\"File path %s is not a valid file or url\" % self.file_path)\n",
      "ValueError: File path 2023_GPT4All_Technical_Report is not a valid file or url\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\maxlo\\anaconda3\\Lib\\site-packages\\gradio\\queueing.py\", line 497, in process_events\n",
      "    response = await self.call_prediction(awake_events, batch)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\maxlo\\anaconda3\\Lib\\site-packages\\gradio\\queueing.py\", line 468, in call_prediction\n",
      "    raise Exception(str(error) if show_error else None) from error\n",
      "Exception: None\n"
     ]
    }
   ],
   "source": [
    "input_pdf_path=gr.components.Textbox(label=\"Provide the PDF File Path \")\n",
    "output_summary=gr.components.Textbox(label=\"Summary\")\n",
    "interface=gr.Interface(\n",
    "    fn=summarize_pdf,\n",
    "    inputs=input_pdf_path,\n",
    "    outputs=output_summary,\n",
    "    title=\"PDF Summarizer\",\n",
    "    description=\"Provide PDF File Path to get the Summary\"\n",
    ").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5CwmA7ja0Ia"
   },
   "source": [
    "#**Step 01: Install All the Required Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZPklrTvUajtS",
    "outputId": "2b678e4f-4ffe-4072-c71d-30d2a6b2f832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/255.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m245.8/255.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.0/255.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.0/289.0 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q gradio openai pypdf tiktoken langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEoR-4Zsbs2-"
   },
   "source": [
    "#**Step 02: Import All the Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "R_jYpVnwbs_S"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "import gradio as gr\n",
    "from langchain import OpenAI, PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HupqeHRhbjAe"
   },
   "source": [
    "#**Step 03: Setting the Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "mtLydxHNbb6z"
   },
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=\"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAR3gEm9b8gA"
   },
   "source": [
    "#**Step 04: Count the Number of Tokens in a String**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "MYVqbHTeb4f7"
   },
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string:str, encoding_name:str) -> int:\n",
    "  #Returns the Number of Token in a text string\n",
    "  encoding = tiktoken.get_encoding(encoding_name)\n",
    "  num_tokens=len(encoding.encode(string))\n",
    "  return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nXbrt5aUdjy4",
    "outputId": "3c625a72-d24b-458d-a1be-92aefb162ba1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_string(\"Hello how are you\", \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNIXavThfJze"
   },
   "source": [
    "#**Step 05: Download the GPT4ALL Technical Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FXcM29m_fR38",
    "outputId": "7fea724b-a38a-4dd5-dda5-405e4f51d99f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-07-18 10:52:22--  https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All_Technical_Report.pdf\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.107.166, 16.182.70.64, 52.216.213.240, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.107.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3651423 (3.5M) [application/pdf]\n",
      "Saving to: ‘2023_GPT4All_Technical_Report.pdf.1’\n",
      "\n",
      "2023_GPT4All_Techni 100%[===================>]   3.48M  17.9MB/s    in 0.2s    \n",
      "\n",
      "2023-07-18 10:52:23 (17.9 MB/s) - ‘2023_GPT4All_Technical_Report.pdf.1’ saved [3651423/3651423]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All_Technical_Report.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhgAp-mPfeRG"
   },
   "source": [
    "#**Step 06: Load, Read and Extract Text From the PDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "9GlkOCQ_gE6v"
   },
   "outputs": [],
   "source": [
    "loader=PyPDFLoader(\"/content/2023_GPT4All_Technical_Report.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "X97KtmFngeRi"
   },
   "outputs": [],
   "source": [
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mj51-DFhgmJ-",
    "outputId": "1a19d430-71f3-465f-ece9-1e1c2f82154e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='GPT4All: Training an Assistant-style Chatbot with Large Scale Data\\nDistillation from GPT-3.5-Turbo\\nYuvanesh Anand\\nyuvanesh@nomic.aiZach Nussbaum\\nzanussbaum@gmail.com\\nBrandon Duderstadt\\nbrandon@nomic.aiBenjamin Schmidt\\nben@nomic.aiAndriy Mulyar\\nandriy@nomic.ai\\nAbstract\\nThis preliminary technical report describes the\\ndevelopment of GPT4All, a chatbot trained\\nover a massive curated corpus of assistant in-\\nteractions including word problems, story de-\\nscriptions, multi-turn dialogue, and code. We\\nopenly release the collected data, data cura-\\ntion procedure, training code, and final model\\nweights to promote open research and repro-\\nducibility. Additionally, we release quantized\\n4-bit versions of the model allowing virtually\\nanyone to run the model on CPU.\\n1 Data Collection and Curation\\nWe collected roughly one million prompt-\\nresponse pairs using the GPT-3.5-Turbo OpenAI\\nAPI between March 20, 2023 and March 26th,\\n2023. To do this, we first gathered a diverse sam-\\nple of questions/prompts by leveraging three pub-\\nlicly available datasets:\\n• The unified chip2 subset of LAION OIG.\\n• Coding questions with a random sub-sample\\nof Stackoverflow Questions\\n• Instruction-tuning with a sub-sample of Big-\\nscience/P3\\nWe chose to dedicate substantial attention to data\\npreparation and curation based on commentary in\\nthe Stanford Alpaca project (Taori et al., 2023).\\nUpon collection of the initial dataset of prompt-\\ngeneration pairs, we loaded data into Atlas for data\\ncuration and cleaning. With Atlas, we removed all\\nexamples where GPT-3.5-Turbo failed to respond\\nto prompts and produced malformed output. This\\nreduced our total number of examples to 806,199\\nhigh-quality prompt-generation pairs. Next, we\\ndecided to remove the entire Bigscience/P3 sub-\\nset from the final training dataset due to its very\\nFigure 1: TSNE visualization of the candidate training\\ndata (Red: Stackoverflow, Orange: chip2, Blue: P3).\\nThe large blue balls (e.g. indicated by the red arrow)\\nare highly homogeneous prompt-response pairs.\\nlow output diversity; P3 contains many homoge-\\nneous prompts which produce short and homoge-\\nneous responses from GPT-3.5-Turbo. This exclu-\\nsion produces a final subset containing 437,605\\nprompt-generation pairs, which is visualized in\\nFigure 2. You can interactively explore the dataset\\nat each stage of cleaning at the following links:\\n• Cleaned with P3\\n• Cleaned without P3 (Final Training Dataset)\\n2 Model Training\\nWe train several models finetuned from an in-\\nstance of LLaMA 7B (Touvron et al., 2023).\\nThe model associated with our initial public re-\\nlease is trained with LoRA (Hu et al., 2021)\\non the 437,605 post-processed examples for four\\nepochs. Detailed model hyper-parameters and\\ntraining code can be found in the associated repos-\\nitory and model training log.', metadata={'source': '/content/2023_GPT4All_Technical_Report.pdf', 'page': 0}),\n",
       " Document(page_content='(a) TSNE visualization of the final training data, ten-colored\\nby extracted topic.\\n(b) Zoomed in view of Figure 2a. The region displayed con-\\ntains generations related to personal health and wellness.\\nFigure 2: The final training data was curated to ensure a diverse distribution of prompt topics and model responses.\\n2.1 Reproducibility\\nWe release all data (including unused P3 genera-\\ntions), training code, and model weights for the\\ncommunity to build upon. Please check the Git\\nrepository for the most up-to-date data, training\\ndetails and checkpoints.\\n2.2 Costs\\nWe were able to produce these models with about\\nfour days work, $800 in GPU costs (rented from\\nLambda Labs and Paperspace) including several\\nfailed trains, and $500 in OpenAI API spend.\\nOur released model, gpt4all-lora, can be trained in\\nabout eight hours on a Lambda Labs DGX A100\\n8x 80GB for a total cost of $100 .\\n3 Evaluation\\nWe perform a preliminary evaluation of our model\\nusing the human evaluation data from the Self-\\nInstruct paper (Wang et al., 2022). We report the\\nground truth perplexity of our model against what\\nis, to our knowledge, the best openly available\\nalpaca-lora model, provided by user chainyo on\\nhuggingface. We find that all models have very\\nlarge perplexities on a small number of tasks, and\\nreport perplexities clipped to a maximum of 100.\\nModels finetuned on this collected dataset ex-\\nhibit much lower perplexity in the Self-Instruct\\nevaluation compared to Alpaca. This evaluation is\\nin no way exhaustive and further evaluation work\\nFigure 3: Model Perplexities. Lower is better. Our\\nmodels achieve stochastically lower ground truth per-\\nplexities than alpaca-lora.\\nremains. We welcome the reader to run the model\\nlocally on CPU (see Github for files) and get a\\nqualitative sense of what it can do.\\n4 Use Considerations\\nThe authors release data and training details in\\nhopes that it will accelerate open LLM research,\\nparticularly in the domains of alignment and inter-\\npretability. GPT4All model weights and data are\\nintended and licensed only for research purposes\\nand any commercial use is prohibited. GPT4All\\nis based on LLaMA, which has a non-commercial\\nlicense. The assistant data is gathered from Ope-\\nnAI’s GPT-3.5-Turbo, whose terms of use pro-', metadata={'source': '/content/2023_GPT4All_Technical_Report.pdf', 'page': 1}),\n",
       " Document(page_content='hibit developing models that compete commer-\\ncially with OpenAI.\\nReferences\\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\\nWeizhu Chen. 2021. Lora: Low-rank adaptation of\\nlarge language models.\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\\nDubois, Xuechen Li, Carlos Guestrin, Percy\\nLiang, and Tatsunori B. Hashimoto. 2023. Stan-\\nford alpaca: An instruction-following llama\\nmodel. https://github.com/tatsu-lab/\\nstanford_alpaca .\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timoth ´ee Lacroix,\\nBaptiste Rozi `ere, Naman Goyal, Eric Hambro,\\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin,\\nEdouard Grave, and Guillaume Lample. 2023.\\nLlama: Open and efficient foundation language\\nmodels.\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\\nisa Liu, Noah A. Smith, Daniel Khashabi, and Han-\\nnaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\\nguage model with self generated instructions.', metadata={'source': '/content/2023_GPT4All_Technical_Report.pdf', 'page': 2})]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MYC4f2bmh8Wt",
    "outputId": "509a85f6-5d88-4d7d-d490-3e79e326727c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='(a) TSNE visualization of the final training data, ten-colored\\nby extracted topic.\\n(b) Zoomed in view of Figure 2a. The region displayed con-\\ntains generations related to personal health and wellness.\\nFigure 2: The final training data was curated to ensure a diverse distribution of prompt topics and model responses.\\n2.1 Reproducibility\\nWe release all data (including unused P3 genera-\\ntions), training code, and model weights for the\\ncommunity to build upon. Please check the Git\\nrepository for the most up-to-date data, training\\ndetails and checkpoints.\\n2.2 Costs\\nWe were able to produce these models with about\\nfour days work, $800 in GPU costs (rented from\\nLambda Labs and Paperspace) including several\\nfailed trains, and $500 in OpenAI API spend.\\nOur released model, gpt4all-lora, can be trained in\\nabout eight hours on a Lambda Labs DGX A100\\n8x 80GB for a total cost of $100 .\\n3 Evaluation\\nWe perform a preliminary evaluation of our model\\nusing the human evaluation data from the Self-\\nInstruct paper (Wang et al., 2022). We report the\\nground truth perplexity of our model against what\\nis, to our knowledge, the best openly available\\nalpaca-lora model, provided by user chainyo on\\nhuggingface. We find that all models have very\\nlarge perplexities on a small number of tasks, and\\nreport perplexities clipped to a maximum of 100.\\nModels finetuned on this collected dataset ex-\\nhibit much lower perplexity in the Self-Instruct\\nevaluation compared to Alpaca. This evaluation is\\nin no way exhaustive and further evaluation work\\nFigure 3: Model Perplexities. Lower is better. Our\\nmodels achieve stochastically lower ground truth per-\\nplexities than alpaca-lora.\\nremains. We welcome the reader to run the model\\nlocally on CPU (see Github for files) and get a\\nqualitative sense of what it can do.\\n4 Use Considerations\\nThe authors release data and training details in\\nhopes that it will accelerate open LLM research,\\nparticularly in the domains of alignment and inter-\\npretability. GPT4All model weights and data are\\nintended and licensed only for research purposes\\nand any commercial use is prohibited. GPT4All\\nis based on LLaMA, which has a non-commercial\\nlicense. The assistant data is gathered from Ope-\\nnAI’s GPT-3.5-Turbo, whose terms of use pro-', metadata={'source': '/content/2023_GPT4All_Technical_Report.pdf', 'page': 1})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoH1yk4Agpu6"
   },
   "source": [
    "#**Step 07: Split the Text into Chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "Gb2XVHtFgzta"
   },
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "mj0a-fmNgz0P"
   },
   "outputs": [],
   "source": [
    "texts=text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p5yRvFzgi91m",
    "outputId": "3570bac4-edff-4963-d989-949e5828527c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o23mXcqWee_O"
   },
   "source": [
    "#**Step 08: Load the OpenAI Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "j1N4A6qyfXmm"
   },
   "outputs": [],
   "source": [
    "llm=OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5nZ5RIqrdvMc",
    "outputId": "997cacca-0e2c-4022-8702-b66596ffa2f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.7, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all', tiktoken_model_name=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "GssjoWN9jWsW"
   },
   "outputs": [],
   "source": [
    "chain=load_summarize_chain(llm, chain_type='map_reduce', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vlGmedBnjWwJ",
    "outputId": "571f9fe2-9336-4e01-8894-8753bb15081e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"GPT4All: Training an Assistant-style Chatbot with Large Scale Data\n",
      "Distillation from GPT-3.5-Turbo\n",
      "Yuvanesh Anand\n",
      "yuvanesh@nomic.aiZach Nussbaum\n",
      "zanussbaum@gmail.com\n",
      "Brandon Duderstadt\n",
      "brandon@nomic.aiBenjamin Schmidt\n",
      "ben@nomic.aiAndriy Mulyar\n",
      "andriy@nomic.ai\n",
      "Abstract\n",
      "This preliminary technical report describes the\n",
      "development of GPT4All, a chatbot trained\n",
      "over a massive curated corpus of assistant in-\n",
      "teractions including word problems, story de-\n",
      "scriptions, multi-turn dialogue, and code. We\n",
      "openly release the collected data, data cura-\n",
      "tion procedure, training code, and final model\n",
      "weights to promote open research and repro-\n",
      "ducibility. Additionally, we release quantized\n",
      "4-bit versions of the model allowing virtually\n",
      "anyone to run the model on CPU.\n",
      "1 Data Collection and Curation\n",
      "We collected roughly one million prompt-\n",
      "response pairs using the GPT-3.5-Turbo OpenAI\n",
      "API between March 20, 2023 and March 26th,\n",
      "2023. To do this, we first gathered a diverse sam-\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"We collected roughly one million prompt-\n",
      "response pairs using the GPT-3.5-Turbo OpenAI\n",
      "API between March 20, 2023 and March 26th,\n",
      "2023. To do this, we first gathered a diverse sam-\n",
      "ple of questions/prompts by leveraging three pub-\n",
      "licly available datasets:\n",
      "• The unified chip2 subset of LAION OIG.\n",
      "• Coding questions with a random sub-sample\n",
      "of Stackoverflow Questions\n",
      "• Instruction-tuning with a sub-sample of Big-\n",
      "science/P3\n",
      "We chose to dedicate substantial attention to data\n",
      "preparation and curation based on commentary in\n",
      "the Stanford Alpaca project (Taori et al., 2023).\n",
      "Upon collection of the initial dataset of prompt-\n",
      "generation pairs, we loaded data into Atlas for data\n",
      "curation and cleaning. With Atlas, we removed all\n",
      "examples where GPT-3.5-Turbo failed to respond\n",
      "to prompts and produced malformed output. This\n",
      "reduced our total number of examples to 806,199\n",
      "high-quality prompt-generation pairs. Next, we\n",
      "decided to remove the entire Bigscience/P3 sub-\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"to prompts and produced malformed output. This\n",
      "reduced our total number of examples to 806,199\n",
      "high-quality prompt-generation pairs. Next, we\n",
      "decided to remove the entire Bigscience/P3 sub-\n",
      "set from the final training dataset due to its very\n",
      "Figure 1: TSNE visualization of the candidate training\n",
      "data (Red: Stackoverflow, Orange: chip2, Blue: P3).\n",
      "The large blue balls (e.g. indicated by the red arrow)\n",
      "are highly homogeneous prompt-response pairs.\n",
      "low output diversity; P3 contains many homoge-\n",
      "neous prompts which produce short and homoge-\n",
      "neous responses from GPT-3.5-Turbo. This exclu-\n",
      "sion produces a final subset containing 437,605\n",
      "prompt-generation pairs, which is visualized in\n",
      "Figure 2. You can interactively explore the dataset\n",
      "at each stage of cleaning at the following links:\n",
      "• Cleaned with P3\n",
      "• Cleaned without P3 (Final Training Dataset)\n",
      "2 Model Training\n",
      "We train several models finetuned from an in-\n",
      "stance of LLaMA 7B (Touvron et al., 2023).\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"• Cleaned with P3\n",
      "• Cleaned without P3 (Final Training Dataset)\n",
      "2 Model Training\n",
      "We train several models finetuned from an in-\n",
      "stance of LLaMA 7B (Touvron et al., 2023).\n",
      "The model associated with our initial public re-\n",
      "lease is trained with LoRA (Hu et al., 2021)\n",
      "on the 437,605 post-processed examples for four\n",
      "epochs. Detailed model hyper-parameters and\n",
      "training code can be found in the associated repos-\n",
      "itory and model training log.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"(a) TSNE visualization of the final training data, ten-colored\n",
      "by extracted topic.\n",
      "(b) Zoomed in view of Figure 2a. The region displayed con-\n",
      "tains generations related to personal health and wellness.\n",
      "Figure 2: The final training data was curated to ensure a diverse distribution of prompt topics and model responses.\n",
      "2.1 Reproducibility\n",
      "We release all data (including unused P3 genera-\n",
      "tions), training code, and model weights for the\n",
      "community to build upon. Please check the Git\n",
      "repository for the most up-to-date data, training\n",
      "details and checkpoints.\n",
      "2.2 Costs\n",
      "We were able to produce these models with about\n",
      "four days work, $800 in GPU costs (rented from\n",
      "Lambda Labs and Paperspace) including several\n",
      "failed trains, and $500 in OpenAI API spend.\n",
      "Our released model, gpt4all-lora, can be trained in\n",
      "about eight hours on a Lambda Labs DGX A100\n",
      "8x 80GB for a total cost of $100 .\n",
      "3 Evaluation\n",
      "We perform a preliminary evaluation of our model\n",
      "using the human evaluation data from the Self-\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"about eight hours on a Lambda Labs DGX A100\n",
      "8x 80GB for a total cost of $100 .\n",
      "3 Evaluation\n",
      "We perform a preliminary evaluation of our model\n",
      "using the human evaluation data from the Self-\n",
      "Instruct paper (Wang et al., 2022). We report the\n",
      "ground truth perplexity of our model against what\n",
      "is, to our knowledge, the best openly available\n",
      "alpaca-lora model, provided by user chainyo on\n",
      "huggingface. We find that all models have very\n",
      "large perplexities on a small number of tasks, and\n",
      "report perplexities clipped to a maximum of 100.\n",
      "Models finetuned on this collected dataset ex-\n",
      "hibit much lower perplexity in the Self-Instruct\n",
      "evaluation compared to Alpaca. This evaluation is\n",
      "in no way exhaustive and further evaluation work\n",
      "Figure 3: Model Perplexities. Lower is better. Our\n",
      "models achieve stochastically lower ground truth per-\n",
      "plexities than alpaca-lora.\n",
      "remains. We welcome the reader to run the model\n",
      "locally on CPU (see Github for files) and get a\n",
      "qualitative sense of what it can do.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"plexities than alpaca-lora.\n",
      "remains. We welcome the reader to run the model\n",
      "locally on CPU (see Github for files) and get a\n",
      "qualitative sense of what it can do.\n",
      "4 Use Considerations\n",
      "The authors release data and training details in\n",
      "hopes that it will accelerate open LLM research,\n",
      "particularly in the domains of alignment and inter-\n",
      "pretability. GPT4All model weights and data are\n",
      "intended and licensed only for research purposes\n",
      "and any commercial use is prohibited. GPT4All\n",
      "is based on LLaMA, which has a non-commercial\n",
      "license. The assistant data is gathered from Ope-\n",
      "nAI’s GPT-3.5-Turbo, whose terms of use pro-\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"hibit developing models that compete commer-\n",
      "cially with OpenAI.\n",
      "References\n",
      "Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\n",
      "Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\n",
      "Weizhu Chen. 2021. Lora: Low-rank adaptation of\n",
      "large language models.\n",
      "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\n",
      "Dubois, Xuechen Li, Carlos Guestrin, Percy\n",
      "Liang, and Tatsunori B. Hashimoto. 2023. Stan-\n",
      "ford alpaca: An instruction-following llama\n",
      "model. https://github.com/tatsu-lab/\n",
      "stanford_alpaca .\n",
      "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\n",
      "Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix,\n",
      "Baptiste Rozi `ere, Naman Goyal, Eric Hambro,\n",
      "Faisal Azhar, Aurelien Rodriguez, Armand Joulin,\n",
      "Edouard Grave, and Guillaume Lample. 2023.\n",
      "Llama: Open and efficient foundation language\n",
      "models.\n",
      "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\n",
      "isa Liu, Noah A. Smith, Daniel Khashabi, and Han-\n",
      "naneh Hajishirzi. 2022. Self-instruct: Aligning lan-\n",
      "guage model with self generated instructions.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\" This technical report describes the development of GPT4All, a chatbot trained on a massive corpus of assistant interactions, such as word problems, story descriptions, multi-turn dialogue, and code. The report also includes the data collection and curation procedure, training code, and model weights to promote open research and reproducibility. Additionally, a quantized 4-bit version of the model was released to allow for running on CPUs.\n",
      "\n",
      "\n",
      "Between March 20 and March 26, 2023, we collected roughly one million prompt-response pairs using the GPT-3.5-Turbo OpenAI API, with the help of three publicly available datasets. After data curation and cleaning, 806,199 high-quality prompt-generation pairs remained. We removed the Bigscience/P3 subset from our final dataset.\n",
      "\n",
      " This text discusses the process involved in cleaning and forming a final training dataset for a prompt-generation model. 806,199 high-quality prompt-generation pairs were initially found, but the Bigscience/P3 subset was removed due to its lack of output diversity. This left 437,605 prompt-generation pairs, which were visualized in Figure 2. The dataset can be explored interactively at two links, and several models were finetuned from an instance of LLaMA 7B (Touvron et al., 2023) for training.\n",
      "\n",
      " This article discusses the training of several models based on an instance of LLaMA 7B, using the 437,605 post-processed examples and the LoRA algorithm. Detailed information on the model hyper-parameters and training code is available in the associated repository and model training log.\n",
      "\n",
      " This paper presents a training dataset that was curated to include a diverse range of topics and model responses. All data, training code, and model weights are released to the public and the model can be trained in 8 hours with a total cost of $100. A preliminary evaluation of the model using human evaluation data from the Self- dataset was conducted.\n",
      "\n",
      " This evaluation of a model finetuned on a collected dataset compared to alpaca-lora showed that the finetuned model had lower perplexities in the Self-Instruct evaluation. Further evaluation is needed and readers are encouraged to run the model locally to get a qualitative sense of its abilities.\n",
      "\n",
      "\n",
      "\n",
      "The authors have released data and training details in order to accelerate open LLM research, particularly in the domains of alignment and interpretability. The GPT4All model weights and data are intended for research purposes only and any commercial use is prohibited. The assistant data is gathered from OpenAI's GPT-3.5-Turbo, whose terms of use need to be followed. The model can be run locally on CPU and instructions can be found on Github.\n",
      "\n",
      " This article discusses three models for large language models which compete commercially with OpenAI: LORA, Stanford Alpaca, and Llama. These models are all designed to be low-rank, open and efficient, and to align language models with self-generated instructions.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' This article presents a large-scale language model, GPT4All, trained on a vast corpus of assistant interactions and released for open research and reproducibility. It covers the data curation and model training process, model weights, and a quantized 4-bit version of the model to run on CPUs. Evaluating the model using human evaluation data from the Self- dataset showed lower perplexities when finetuned compared to competing models. The data and training details are released to promote open LLM research.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6X6o5ZO7j8IW"
   },
   "source": [
    "#**Step 09: Create a Simple Gradio UI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "doQU8DXOlIN6"
   },
   "outputs": [],
   "source": [
    "def summarize_pdf(pdf_file_path):\n",
    "  loader=PyPDFLoader(pdf_file_path)\n",
    "  docs=loader.load()\n",
    "  text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "  texts=text_splitter.split_documents(docs)\n",
    "  chain=load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "  summary=chain.run(texts)\n",
    "  return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "id": "nbS8VjXhjWzh",
    "outputId": "17fc4f64-3c47-4b92-b761-c83ccb22ab1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "Running on public URL: https://e760e1ee3639f00ff2.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://e760e1ee3639f00ff2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_pdf_path=gr.components.Textbox(label=\"Provide the PDF File Path \")\n",
    "output_summary=gr.components.Textbox(label=\"Summary\")\n",
    "\n",
    "interface=gr.Interface(\n",
    "    fn=summarize_pdf,\n",
    "    inputs=input_pdf_path,\n",
    "    outputs=output_summary,\n",
    "    title=\"PDF Summarizer\",\n",
    "    description=\"Provide PDF File Path to get the Summary\"\n",
    ").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bA0nVPWzjW26"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqd16LZmjW6W"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xLp-6_B3jW9T"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwAYNkUGeoPz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
